{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ddd40c6",
      "metadata": {
        "id": "4ddd40c6"
      },
      "source": [
        "# Mentahan"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# When you run this cell, a prompt will appear asking you to authorize Google Drive access.\n",
        "# Click the link, select your Google account, and allow the necessary permissions.\n",
        "# Then, copy the authorization code provided and paste it back into the input box in Colab.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kCoU59X6zo2M",
        "outputId": "4b414e8b-05f0-4d43-9904-705cd051cdf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "kCoU59X6zo2M",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def read_signal_from_txt(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = f.readlines()\n",
        "    data = [float(line.strip()) for line in data]\n",
        "    return data\n",
        "\n",
        "def load_dataset_from_folder(folder_path, label):\n",
        "    rows = []\n",
        "    for filename in sorted(os.listdir(folder_path)):\n",
        "        if filename.endswith('.txt'):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            signal = read_signal_from_txt(file_path)\n",
        "            if len(signal) == 4097:\n",
        "                rows.append([label] + signal)\n",
        "    return rows\n",
        "\n",
        "# Ganti path ke lokasi dataset kamu\n",
        "base_path = \"C:/Users/syahla/Downloads/z\"\n",
        "\n",
        "# Folder-folder kategori EEG\n",
        "categories = {\n",
        "    'Z': 0,  # non-epileptic\n",
        "    'S': 1   # epileptic\n",
        "}\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for folder, label in categories.items():\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    data = load_dataset_from_folder(folder_path, label)\n",
        "    all_data.extend(data)\n",
        "\n",
        "# Simpan ke CSV\n",
        "df = pd.DataFrame(all_data)\n",
        "df.to_csv(\"EEG_epilepsy_dataset.csv\", index=False, header=[\"label\"] + [f\"v{i}\" for i in range(1, 4098)])\n",
        "\n",
        "print(\"Konversi selesai. Dataset disimpan di EEG_epilepsy_dataset.csv\")\n"
      ],
      "metadata": {
        "id": "EWPLv_EY-xAx"
      },
      "id": "EWPLv_EY-xAx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c377f8",
      "metadata": {
        "id": "70c377f8"
      },
      "outputs": [],
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Load Data from All 5 Sets\n",
        "# -------------------------------\n",
        "\n",
        "def load_data_from_mat(file_path, label):\n",
        "    mat = scipy.io.loadmat(file_path)\n",
        "    data = mat['data']  # Data assumed to be under 'data' key\n",
        "    return data, np.full((data.shape[0],), label)\n",
        "\n",
        "# Path to directory containing the 5 .mat files\n",
        "data_dir = '/content/drive/MyDrive/dataset eeg'  # Ubah sesuai direktori kamu\n",
        "\n",
        "# File names (gunakan nama file asli seperti \"Z.mat\", \"O.mat\", dst.)\n",
        "file_labels = {\n",
        "    'Z.mat': 0,  # non-seizure\n",
        "    'O.mat': 0,\n",
        "    'N.mat': 0,\n",
        "    'F.mat': 0,\n",
        "    'S.mat': 1   # seizure\n",
        "}\n",
        "\n",
        "X, y = [], []\n",
        "\n",
        "for fname, label in file_labels.items():\n",
        "    data, labels = load_data_from_mat(os.path.join(data_dir, fname), label)\n",
        "    X.append(data)\n",
        "    y.append(labels)\n",
        "\n",
        "X = np.vstack(X)\n",
        "y = np.hstack(y)\n",
        "\n",
        "print(\"Data shape:\", X.shape)  # (500, 4097)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Preprocessing\n",
        "# -------------------------------\n",
        "\n",
        "# Normalisasi\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape untuk CNN input [samples, time steps, channels]\n",
        "X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Build CNN Model\n",
        "# -------------------------------\n",
        "\n",
        "model = Sequential([\n",
        "    Conv1D(32, kernel_size=5, activation='relu', input_shape=(4097, 1)),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Conv1D(64, kernel_size=5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Train Model\n",
        "# -------------------------------\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=16, validation_split=0.2)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Evaluate Model\n",
        "# -------------------------------\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# 6. Visualisasi\n",
        "# -------------------------------\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Val')\n",
        "plt.title(\"CNN Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}